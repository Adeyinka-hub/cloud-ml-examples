{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving model performance with xfeat, RAPIDS and Optuna\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Feature Engineering is the processing of transforming raw data into features that can represent the underlying patterns of the data better. They can help boost the accuracy by a great deal and improve the ability of the model to generalise on unseen data. Every data scientist knows the importance feature engineering. Spending some time thinking about how best to apply and combine the available features can be very meaningful. \n",
    "\n",
    "Hyper parameter Optimisation is another such process which can help complement a good model by tuning it's hyperparameters, which can have a tremendous impact on the accuracy of the model. The time and resources required for these processes are generally the reason they're overlooked. \n",
    "\n",
    "With xfeat, RAPIDS and Optuna - we aim to bridge these gaps and elevate the performance. \n",
    "\n",
    "## What is Optuna?\n",
    "[Optuna](https://github.com/optuna/optuna) is a lightweight framework for automatic hyperparameter optimization. It provides a define-by-run API, which makes it easy to adapt to any already existing code that we have and enables high modularity and the flexibility to construct hyperparameter spaces dynamically. By simply wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space. As we'll see in this notebook.\n",
    "\n",
    "## What is xfeat?\n",
    "[xfeat](https://github.com/pfnet-research/xfeat) is a feature engineering & exploration library using GPUs and Optuna. It provides a scikit-learn-like API for feature engineering with support for Pandas, cuDF dataframes and cuPy arrays. \n",
    "\n",
    "## What is MLflow?\n",
    "[MLflow](https://mlflow.org/https://mlflow.org/) is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry.\n",
    "\n",
    "## What is RAPIDS?\n",
    "[RAPIDS](https://rapids.ai/about.html) framework  provides a library suite that can execute end-to-end data science pipelines entirely on GPUs.  The libraries in the framework include [cuDF](https://github.com/rapidsai/cudf) - a GPU Dataframe with pandas-like API, [cuML](https://github.com/rapidsai/cuml) - implement machine learning algorithms that provide a scikit-learn-like API and many more. You can learn more [here](https://github.com/rapidsai).\n",
    "\n",
    "In this notebook, we'll show how one can use these tools together to develop and improve a machine learning model. We'll use Airlines dataset (20M rows) to predict if a flight will be delayed or not. We'll explore how to use Optuna with RAPIDS and the speedups that we can achieve with the integration of these, and to see the improvements with GPU speedups, we have included a pandas version to run on CPU. A table summarizing the results is available at the end of the notebook.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "You need to have the following libraries installed - \n",
    "\n",
    "- MLFlow, Optuna, plotly, kaleido\n",
    "\n",
    "Run the following in a new cell to install these packages:\n",
    "\n",
    "```\n",
    "!pip install mlflow\n",
    "!pip install optuna\n",
    "!pip install plotly\n",
    "!pip install kaleido\n",
    "```\n",
    "\n",
    "- To install xfeat, follow the instructions from their [repository](https://github.com/pfnet-research/xfeat)\n",
    "\n",
    "Restart the kernel after you've installed the packages and then run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cupy\n",
    "import cudf\n",
    "import cuml\n",
    "from cuml import LogisticRegression\n",
    "from cuml.metrics import roc_auc_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from  mlflow.tracking import MlflowClient\n",
    "\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "from optuna.study import StudyDirection\n",
    "from optuna.trial import TrialState\n",
    "from optuna import type_checking\n",
    "\n",
    "import xfeat\n",
    "from xfeat.pipeline import Pipeline\n",
    "from xfeat.num_encoder import SelectNumerical\n",
    "from xfeat.selector import ChiSquareKBest\n",
    "from xfeat.optuna_selector import KBestThresholdExplorer\n",
    "from functools import partial\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from cuml.preprocessing.LabelEncoder import LabelEncoder\n",
    "from cuml.preprocessing.TargetEncoder import TargetEncoder\n",
    "from xfeat import ArithmeticCombinations, Pipeline, SelectNumerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "# Helping time blocks of code\n",
    "@contextmanager\n",
    "def timed(txt):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"%32s time:  %8.5f\" % (txt, t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Configuration\n",
    "\n",
    "For tracking the hyperparameter optimisation expriments, we will use MLFlow. In the next cell, the required variables are set up along with the callback class `RAPIDSMLflowCallback` that we will pass to Optuna for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI='sqlite:////tmp/mlflow-db.sqlite'\n",
    "MLFLOW_MODEL_ID = \"rapids-optuna-airline\"\n",
    "\n",
    "def get_latest_mlflow_model(tracking_uri, model_id):\n",
    "    client = MlflowClient(tracking_uri=tracking_uri, registry_uri=tracking_uri)\n",
    "    model = client.get_registered_model(model_id)\n",
    "    latest_model = model.latest_versions[0]\n",
    "\n",
    "    return f\"MLFLOW_TRACKING_URI={tracking_uri} mlflow models serve --no-conda -m models:/{model_id}/{latest_model.version} -p 56767\"\n",
    "\n",
    "## Custom callback, for additional flexibility, based on MLflowCallback\n",
    "class RAPIDSMLflowCallback(object):\n",
    "    def __init__(self, tracking_uri: str = \"sqlite:////tmp/mlflow-db.sqlite\",\n",
    "                 experiment_name: str = \"RAPIDS-Optuna\",\n",
    "                 metric_name=\"value\"):\n",
    "        self._tracking_uri = tracking_uri\n",
    "        self._experiment_name = experiment_name\n",
    "        self._metric_name = metric_name\n",
    "        \n",
    "    def __call__(self, study, trial):\n",
    "        if (self._tracking_uri is not None):\n",
    "            mlflow.set_tracking_uri(self._tracking_uri)\n",
    "        \n",
    "        eid = mlflow.set_experiment(self._experiment_name)\n",
    "        with mlflow.start_run(run_name=f\"Trial: {trial.number}\", experiment_id=eid, nested=True):\n",
    "            trial_value = trial.value if trial.value is not None else float(\"nan\")\n",
    "            mlflow.log_metric(self._metric_name, trial_value)\n",
    "            \n",
    "            mlflow.log_params(trial.params)\n",
    "\n",
    "            tags = {}\n",
    "            tags[\"number\"] = str(trial.number)\n",
    "            tags[\"datetime_start\"] = str(trial.datetime_start)\n",
    "            tags[\"datetime_complete\"] = str(trial.datetime_complete)\n",
    "            tags['RAPIDS cuDF Version'] = str(cudf.__version__)\n",
    "            tags['RAPIDS cuML Version'] = str(cuml.__version__)\n",
    "            tags['SKlearn Version'] = str(sklearn.__version__)\n",
    "\n",
    "            trial_state = trial.state\n",
    "            if (isinstance(trial_state, TrialState)):\n",
    "                tags['state'] = str(trial_state).split('.')[-1]\n",
    "            \n",
    "            # Set direction and convert it to str and remove the common prefix.\n",
    "            study_direction = study.direction\n",
    "            if isinstance(study_direction, StudyDirection):\n",
    "                tags[\"direction\"] = str(study_direction).split(\".\")[-1]\n",
    "\n",
    "            tags.update(trial.user_attrs)\n",
    "            distributions = {\n",
    "                (k + \"_distribution\"): str(v) for (k, v) in trial.distributions.items()\n",
    "            }\n",
    "            tags.update(distributions)\n",
    "\n",
    "            # This is a temporary fix on Optuna side. It avoids an error with user\n",
    "            # attributes that are too long. It should be fixed on MLflow side later.\n",
    "            # When it is fixed on MLflow side this codeblock can be removed.\n",
    "            # see https://github.com/optuna/optuna/issues/1340\n",
    "            # see https://github.com/mlflow/mlflow/issues/2931\n",
    "            max_mlflow_tag_length = 5000\n",
    "            for key, value in tags.items():\n",
    "                value = str(value)  # make sure it is a string\n",
    "                if len(value) > max_mlflow_tag_length:\n",
    "                    tags[key] = textwrap.shorten(value, max_mlflow_tag_length)\n",
    "\n",
    "            mlflow.set_tags(tags) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The following functions are defined to perform a few feature engineering tasks on the data. The `feature_engineering` function is called on the dataframe `df`, in this function we perform a simple Arithmetic Combinations on the numerical columns that adds two columns to create a new one. We specify the `operator` and `r` - r is used to indicate how many columns need to be combined.\n",
    "\n",
    "Then we call `categorical_encoding` which converts the categorical columns to numerical ones and then performs `target_encoding`. Target Encoding replaces the value with the target mean. This is helpful in classification problem to boost the model accuracy. Find more resources at the end of the notebook.\n",
    "\n",
    "You'll also notice we use `Pipeline` from xfeat to combine two or more feature engineering tasks together. This is useful to concatenate encoders sequentially.\n",
    "\n",
    "Read more about Feature Encoding and Pipelining with xfeat [here](https://github.com/pfnet-research/xfeat/blob/master/_docs/feature_encoding.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering and return a new df with engineered features\n",
    "    \"\"\"\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        df_train, df_test, y_train, y_test = train_test_split(df, \"ArrDelayBinary\",random_state=np.random.seed(0), shuffle=True)\n",
    "    else:\n",
    "        from sklearn.model_selection import train_test_split as sk_split\n",
    "        df_train, df_test, y_train, y_test = sk_split(df[df.columns.difference([\"ArrDelayBinary\"])], df[\"ArrDelayBinary\"],random_state=np.random.seed(0), shuffle=True)\n",
    "\n",
    "    # Xfeat's internal fold mechanism creates RangeIndex references, so we need to do an index reset on our data frames.\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    # Need to do this to ensure we are appropriately assigning the split values\n",
    "    df_train[\"ArrDelayBinary\"] = y_train\n",
    "    df_test[\"ArrDelayBinary\"] = y_test\n",
    "    \n",
    "    if isinstance(df_train, cudf.DataFrame):\n",
    "        cat_cols = [\"Dest\", \"Origin\", \"UniqueCarrier\"]\n",
    "        for col in cat_cols:\n",
    "            enc = LabelEncoder()\n",
    "            df_train[col] = enc.fit_transform(df_train[col])\n",
    "            df_test[col] = enc.transform(df_test[col])\n",
    "            del enc\n",
    "\n",
    "        SMOOTH = 0.001\n",
    "        SPLIT = 'interleaved'\n",
    "\n",
    "        for col in cat_cols:\n",
    "            out_col = f'{col}_TE'\n",
    "            # Using cuml Target Encoder\n",
    "            enc = TargetEncoder(n_folds=5, smooth=SMOOTH, split_method=SPLIT)\n",
    "            enc.fit(df_train[col], df_train['ArrDelayBinary'])\n",
    "            df_train[out_col] = enc.transform(df_train[col])\n",
    "            df_test[out_col] = enc.transform(df_test[col])\n",
    "            del enc\n",
    "\n",
    "        encoder = Pipeline([\n",
    "                            SelectNumerical(),\n",
    "                            ArithmeticCombinations(exclude_cols=[\"ArrDelayBinary\"],\n",
    "                                                   drop_origin=False,\n",
    "                                                   operator=\"+\",\n",
    "                                                   r=2,\n",
    "                                                   output_suffix=\"_plus\")\n",
    "                        ])\n",
    "\n",
    "        df_train = encoder.fit_transform(df_train)\n",
    "        df_test = encoder.transform(df_test)\n",
    "        df = cudf.concat([df_train, df_test], sort=False)\n",
    "        \n",
    "    else:\n",
    "        from xfeat.cat_encoder import LabelEncoder as xLabel\n",
    "        from xfeat.cat_encoder import TargetEncoder as xTarget\n",
    "        encoder = Pipeline([\n",
    "                            xLabel(output_suffix=\"\"),\n",
    "                            xTarget(target_col=\"ArrDelayBinary\", output_suffix=\"\"),\n",
    "                            SelectNumerical(),\n",
    "                            ArithmeticCombinations(exclude_cols=[\"ArrDelayBinary\"],\n",
    "                                                   drop_origin=False,\n",
    "                                                   operator=\"+\",\n",
    "                                                   r=2,\n",
    "                                                   output_suffix=\"_plus\")\n",
    "                        ])\n",
    "        df_train = encoder.fit_transform(df_train)\n",
    "        df_test = encoder.transform(df_test)\n",
    "        df = pd.concat([df_train, df_test], sort=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Hyper parameter Optimisation\n",
    "\n",
    "Now that we have some new features, how do we know they are relevant for the task or represent anything meaningful? We use the feature selection process to do this. This helps in selection of a subset of features that are  most informative. This helps in simplifying the problem and ensures that we aren't overloading the system with unimportant features. Optuna provides a way to choose a `selector` which accepts a `Pipeline` object from xfeat. You can see in the `feature_selection` function we define a Pipeline that takes in an Explorer and a Selection Algorithm (`ChiSquareKBest`). We pass this to an Optuna Study object, along with an Objective function\n",
    "\n",
    "Chi squared tests are used to test the independence of two events. For Feature Selection, we aim to select feature, which are highly dependent on the response. This way, we can get features that will best determine the outcome.\n",
    "\n",
    "### Objective Function\n",
    "The objective function will be the one we optimize in Optuna Study. Objective funciton tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`.\n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective remains constant over different samplers, which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll use TPESampler for this demo, but feel free to try different samplers to notice the chnages in performance.\n",
    "\n",
    "\n",
    "### HPO Trials and Study\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. Put simply, a trial is a single call of the objective function while a set of trials make up a study. We will pick the optimal performing trial from a study to get the best parameters that were used in that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_and_eval(df, penalty='l2', C=1.0, l1_ratio=\"None\", fit_intercept=True, selector=None, return_model=False):\n",
    "    # Splitting data and prepping for selector fit\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        X_train,  X_test, y_train, y_test = train_test_split(df,\n",
    "                                                             \"ArrDelayBinary\",\n",
    "                                                             random_state=np.random.seed(0),\n",
    "                                                             shuffle=True)\n",
    "    else:\n",
    "        from sklearn.model_selection import train_test_split as sk_split\n",
    "        X_train,  X_test, y_train, y_test = sk_split(df[df.columns.difference([\"ArrDelayBinary\"])],\n",
    "                                                     df[\"ArrDelayBinary\"],\n",
    "                                                     random_state=np.random.seed(0),\n",
    "                                                     shuffle=True)\n",
    "    # Xfeat's internal fold mechanism creates RangeIndex references, so we need to do an index reset on our data frames.\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    if selector:\n",
    "        # For the selector, the label also needs to be in the DF\n",
    "        X_train[\"ArrDelayBinary\"] = y_train\n",
    "        X_test[\"ArrDelayBinary\"] = y_test\n",
    "\n",
    "        X_train = selector.fit_transform(X_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "\n",
    "    # Train and get accuracy\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        classifier = LogisticRegression(penalty=penalty,\n",
    "                                        C=C,\n",
    "                                        l1_ratio=l1_ratio,\n",
    "                                        fit_intercept=fit_intercept,\n",
    "                                        max_iter=10000)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict_proba(X_test.values)[:, 1]\n",
    "        y_pred = y_pred.astype(y_test.dtype)\n",
    "        score = roc_auc_score(y_test, y_pred)\n",
    "    else:\n",
    "        from sklearn.linear_model import LogisticRegression as sk_log\n",
    "        classifier = sk_log(solver='saga',\n",
    "                            penalty=penalty,\n",
    "                                        C=C,\n",
    "                                        l1_ratio=l1_ratio,\n",
    "                                        fit_intercept=fit_intercept,\n",
    "                                        max_iter=10000)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        from sklearn.metrics import roc_auc_score as sk_auc\n",
    "        score = sk_auc(y_test.astype('float64'), y_pred)\n",
    "    \n",
    "    \n",
    "    if (return_model):\n",
    "        if isinstance(X_test, cudf.DataFrame):\n",
    "            return score, classifier, infer_signature(X_test.to_pandas(), cupy.asnumpy(y_pred))\n",
    "        else:\n",
    "            return score, classifier, infer_signature(X_test, y_pred)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def objective(df, selector, trial):\n",
    "    \"\"\"\n",
    "    Performs the training and evaluation of the set of parameters and subset of features using selector.\n",
    "    \"\"\"\n",
    "    selector.set_trial(trial)\n",
    "    \n",
    "    # Select Params\n",
    "    C = trial.suggest_uniform(\"C\", 0 , 7.0)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", ['l1', 'none', 'l2'])\n",
    "    l1_ratio = trial.suggest_uniform(\"l1_ratio\", 0 , 1.0)\n",
    "    fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    \n",
    "    score = train_and_eval(df,\n",
    "                           penalty=penalty,\n",
    "                           C=C,\n",
    "                           l1_ratio=l1_ratio,\n",
    "                           fit_intercept=fit_intercept,\n",
    "                           selector=selector)\n",
    "    return score\n",
    "\n",
    "def feature_selection(df, experiment_name):\n",
    "    \"\"\"\n",
    "    Defines the Pipeline and performs the optuna opt\n",
    "    \"\"\"\n",
    "    artifact_path = \"rapids-optuna-airline\"\n",
    "    selector = Pipeline(\n",
    "        [\n",
    "            SelectNumerical(),\n",
    "            KBestThresholdExplorer(ChiSquareKBest(target_col=\"ArrDelayBinary\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    mlfcb = RAPIDSMLflowCallback(\n",
    "        tracking_uri=MLFLOW_TRACKING_URI,\n",
    "        experiment_name=experiment_name,\n",
    "        metric_name='auc')\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f\"Optuna-HPO:{study.study_name}\"):\n",
    "        study.optimize(partial(objective, df, selector), n_trials=N_TRIALS, callbacks=[mlfcb])\n",
    "        \n",
    "        selector.from_trial(study.best_trial)\n",
    "        selected_cols = selector.get_selected_cols()\n",
    "        \n",
    "        df_select = df[selected_cols]\n",
    "        df_select['ArrDelayBinary'] = df['ArrDelayBinary']\n",
    "        \n",
    "        params = study.best_params\n",
    "        score, classifier, signature = train_and_eval(df_select,\n",
    "                      C=params['C'],\n",
    "                      penalty=params['penalty'],\n",
    "                      l1_ratio=params['l1_ratio'],\n",
    "                      fit_intercept=params['fit_intercept'],\n",
    "                      return_model=True)\n",
    "        \n",
    "        with mlflow.start_run(run_name='Final Classifier', nested=True):\n",
    "            mlflow.log_metric('accuracy', score)\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.sklearn.log_model(classifier,\n",
    "                                 signature=signature,\n",
    "                                 artifact_path=artifact_path,\n",
    "                                 registered_model_name=\"rapids-optuna-airline\",\n",
    "                                 conda_env='conda/conda.yaml')\n",
    "\n",
    "    return study, df_select.reset_index(drop=True), classifier, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Experiment Variables\n",
    "\n",
    "Change the `INPUT_FILE` to correspond to the path in your local system and select the number of rows and trials to run the experiment for. \n",
    "\n",
    "NOTE: It is not recommended to run the CPU version for more than 100,000 rows. To avoid this, we set a glad `cpu_run` based on `N_ROWS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"/home/data/airline_data/airline_small.parquet\"\n",
    "\n",
    "N_ROWS = 1000000\n",
    "N_TRIALS = 100\n",
    "\n",
    "cpu_run = N_ROWS <= 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cpu_run:\n",
    "    with timed(\"Complete Pandas run\"):\n",
    "        df_pandas = pd.read_parquet(INPUT_FILE)[:N_ROWS]\n",
    "        print(\"Default scikit-learn \", train_and_eval(df_pandas))\n",
    "\n",
    "        with timed(\"Pandas FE\"):\n",
    "            df_pandas_fe = feature_engineering(df_pandas)\n",
    "            df_pandas_fe[\"ArrDelayBinary\"] = df_pandas_fe[\"ArrDelayBinary\"].astype('float64')\n",
    "            score = train_and_eval(df_pandas_fe)\n",
    "        print(\"After feature eng: \", score)\n",
    "\n",
    "        with timed(\"FS + Optuna PANDAS\"):\n",
    "            # Disable Alembic driver, used by MLflow, from logging INFO messages to the command line.\n",
    "            logging.getLogger('alembic').setLevel(logging.CRITICAL)\n",
    "            study, df_select, _, score = feature_selection(df_pandas_fe, experiment_name='Optuna_CPU')\n",
    "        print(\"After Feature selection \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Run\n",
    "\n",
    "Now, let's run the RAPIDS version by first reading in the data as cudf DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default performance:  0.5890865921974182\n",
      "After feature eng:  0.6576112508773804\n",
      "                              FE time:  47.61658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-08 20:03:41,835] A new study created in memory with name: no-name-6be754fb-cada-44ae-8098-c3e11256a346\n",
      "[I 2020-09-08 20:04:19,078] Trial 0 finished with value: 0.6597296595573425 and parameters: {'C': 4.255693972947152, 'penalty': 'none', 'l1_ratio': 0.7510359991911492, 'fit_intercept': True, 'KBestThresholdExplorer.k': 87.0}. Best is trial 0 with value: 0.6597296595573425.\n",
      "[I 2020-09-08 20:04:35,510] Trial 1 finished with value: 0.6213146448135376 and parameters: {'C': 5.158597790137891, 'penalty': 'l2', 'l1_ratio': 0.5765178656159643, 'fit_intercept': False, 'KBestThresholdExplorer.k': 27.0}. Best is trial 0 with value: 0.6597296595573425.\n",
      "[I 2020-09-08 20:05:16,661] Trial 2 finished with value: 0.5983195304870605 and parameters: {'C': 2.427516961038565, 'penalty': 'l1', 'l1_ratio': 0.8474938157025717, 'fit_intercept': True, 'KBestThresholdExplorer.k': 53.0}. Best is trial 0 with value: 0.6597296595573425.\n",
      "[I 2020-09-08 20:05:38,572] Trial 3 finished with value: 0.6358400583267212 and parameters: {'C': 6.920346265953794, 'penalty': 'none', 'l1_ratio': 0.8679492815371123, 'fit_intercept': True, 'KBestThresholdExplorer.k': 32.0}. Best is trial 0 with value: 0.6597296595573425.\n"
     ]
    }
   ],
   "source": [
    "with timed(\"Complete RAPIDS workflow\"):\n",
    "    df_ = cudf.read_parquet(INPUT_FILE)[:N_ROWS]\n",
    "    # Can't handle nagative values, yet\n",
    "    df_ = df_.drop([\"ActualElapsedTime\"], axis=1)\n",
    "    print(\"Default performance: \", train_and_eval(df_))\n",
    "\n",
    "    # We cast to objects for categorical  and target encoding\n",
    "    df_[\"UniqueCarrier\"] = df_[\"UniqueCarrier\"].astype(\"object\")\n",
    "    df_[\"Origin\"] = df_[\"Origin\"].astype(\"object\")\n",
    "    df_[\"Dest\"] = df_[\"Dest\"].astype(\"object\")\n",
    "\n",
    "    with timed(\"FE\"):\n",
    "        df_feature_eng = feature_engineering(df_)\n",
    "        df_feature_eng[\"ArrDelayBinary\"] = df_feature_eng[\"ArrDelayBinary\"].astype('float64')\n",
    "        score = train_and_eval(df_feature_eng)\n",
    "        print(\"After feature eng: \", score)\n",
    "    \n",
    "    with timed(\"FS + Optuna\"):\n",
    "        # Disable Alembic driver, used by MLflow, from logging INFO messages to the command line.\n",
    "        logging.getLogger('alembic').setLevel(logging.CRITICAL)\n",
    "        study, df_select, best_clf, score = feature_selection(df_feature_eng, experiment_name='Optuna_SingleGPU')\n",
    "        print(\"Best score after Feature Selection + Optuna: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The details of the best trial \", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summarization\n",
    "\n",
    "We noticed that Feature Engineering alone takes 28.47 seconds on CPU vs 7.78 seconds on GPU, yieling a 4x speed up.\n",
    "\n",
    "Performing Feature engineering and Selection boosts the AUC score from 0.61 to 0.72. Byy repeating this task on a larger portion of the data, a wider search space, we would be able to achieve a better improvement.\n",
    "\n",
    "From our experiemnts, GPU runs are faster for 100,000 rows (and 10 trials) and we are able to obtain <b>4.8x</b> speedups. For more performance improvements, you are encouraged to increased the number of rows and/or number of trials. This will result in a big boost in accuracy. Keep in mind, you do not want to run the experiment on CPU with a larger number of rows, as this will result in the kernel crashing.\n",
    "\n",
    "|Number of rows| Trials| CPU | GPU|\n",
    "|-|-|-|-|\n",
    "|100,000|10|389.81|80.33|\n",
    "|1,000,000|100|-|TBD|\n",
    "|10,000,000|100|-|TBD|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's look at some graphs to understand and visualize what we achieved in this notebook. \n",
    "\n",
    "The graph below shows the importance of a feature for the performance. We see that the `penalty` set in Logistic Regression and `K` from the Chi-sqaure test have the highest importance in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "f = optuna.visualization.plot_param_importances(study)\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a slice plot to better understand the parameter relationships. We see how the change in the parameter affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = optuna.visualization.plot_slice(study, params=['l1_ratio', 'C', 'KBestThresholdExplorer.k', 'penalty', 'fit_intercept'])\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of all trials in the study to see how the performance improvements took place within the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = optuna.visualization.plot_optimization_history(study)\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have seen the performance improvement, let's look at how we can retrieve this model with MLFlow.\n",
    "\n",
    "### Launch our optimized model within the MLflow framework.\n",
    "Run the code block below to identify the most recently registered model, with the 'rapids-optuna-airline' tag; after identifying the latest model version, run the code below in a separate terminal, and wait for it to fully load your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Run the command below in a terminal, and wait for it to load your model:\\n\\n  \\\n",
    "      {get_latest_mlflow_model(MLFLOW_TRACKING_URI, MLFLOW_MODEL_ID)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to the following:\n",
    "\n",
    "```shell\n",
    "2020/07/27 13:59:49 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n",
    "2020/07/27 13:59:49 INFO mlflow.pyfunc.backend: === Running command 'source /anaconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-3335621df6011b1847d2555b195418d4496e5ffd 1>&2 && gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Starting gunicorn 20.0.4\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Listening at: http://127.0.0.1:5000 (23779)\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Using worker: sync\n",
    "[2020-07-27 13:59:50 -0600] [23788] [INFO] Booting worker with pid: 23788\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='localhost'\n",
    "port='56767'\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"format\": \"pandas-split\"\n",
    "}\n",
    "\n",
    "data = { \n",
    "    \"columns\": [\"Year\", \"Month\", \"DayofMonth\", \"DayofWeek\", \"CRSDepTime\", \"CRSArrTime\", \"UniqueCarrier\",\n",
    "                \"FlightNum\", \"ActualElapsedTime\", \"Origin\", \"Dest\", \"Distance\", \"Diverted\"],\n",
    "    \"data\": [[1987, 10, 1, 4, 1, 556, 0, 190, 247, 202, 162, 1846, 0]]\n",
    "}\n",
    "\n",
    "while (True):\n",
    "    try:\n",
    "        resp = requests.post(url=\"http://%s:%s/invocations\" % (host, port), data=json.dumps(data), headers=headers)\n",
    "        print('Classification: %s' % (\"ON-Time\" if resp.text == \"[0.0]\" else \"LATE\"))\n",
    "        break\n",
    "    except Exception as e:\n",
    "        errmsg = \"Caught exception attempting to call model endpoint: %s\" % e\n",
    "        print(errmsg)\n",
    "        print(\"... Sleeping ...\")\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "[How to Win a DS Kaggle competition](https://www.coursera.org/learn/competitive-data-science)\n",
    "\n",
    "[Target Encoding and Bayesian Target Encoding](https://towardsdatascience.com/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c)\n",
    "\n",
    "[Learn more about using MLFlow with RAPIDS](https://github.com/mlflow/mlflow/tree/master/examples/rapids/mlflow_project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
