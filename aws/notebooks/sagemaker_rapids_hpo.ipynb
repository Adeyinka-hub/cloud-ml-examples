{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!WIP: Please suggest edits/improvments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Optimization with NVIDIA RAPIDS + AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying domain knowledge, intuition, and experimentation to build a successful model, data scientists typically run hyper-parameter-optimization (HPO) to find a champion model and reach highest performance before deploying to production. \n",
    "\n",
    "HPO searches over models by trying different settings of 'architecture parameters,' parameters not usually optimized by the learning algorithm -- i.e., *maximum depth* and *number-of-trees* in a random forest model, or the *number-of-layers* and *neurons-per-layer* of a neural network. \n",
    "\n",
    "Often HPO can improve the generalization quality of a model by 5-15% relative to hand tuned or default model parameters. But there is a problem, HPO is very computationally expensive (we are searching over model architectures not just individual parameters) and can be very slow.\n",
    "\n",
    "In this notebook we show how we can overcome the computational complexity of HPO by combining two superpowers -- the *scaling power* of the cloud, and the *speed* of the GPU. By using these two super-powers we can vastly accelerate HPO, and best of all you can use these superpowers too! Once you've gone through this content you should be able to plug in custom code and data so you can accelerate HPO on **your ML problem**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification of Airline Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll be leveraging the RAPIDS **cuml.RandomForest** classifier model to try to predict airline arrival delays (see the Dataset section below for more details). To find the best performing model we'll search across three hyper-parameters that control the architecture of the Random Forest \n",
    "\n",
    "- **maximum_depth**: the maximum possible depth of any tree\n",
    "- **n_estimators**: the number of trees in the forest\n",
    "- **max_features**: the fraction of features used to determine splits in the trees\n",
    "\n",
    "<img src='figures/tree_depth.png'>\n",
    "<center>Sample Decision Tree of max_depth = 7</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it Works: HPO on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker provides a work orchestrator for HPO. Given an Estimator object ( essentially containerized model code -- more on this soon), data, and hyper-parameter ranges SageMaker will use a search strategy to try various combinations of hyper-parameters (i.e., experiment) within the admissable ranges and report back on their performance, ultimately reporting on the best performing combination.\n",
    "\n",
    "Currently SageMaker supports **Random** and **Bayesian** search strategies. \n",
    "\n",
    "- The **Random** strategy is as its name implies, randomly sampling in the possible ranges with no concern for past experiments.\n",
    "\n",
    "- The **Bayesian** strategy tries several parallel experiments and then uses regression to pick the next batch of hyper-parameters.\n",
    "\n",
    "In this notebook we'll be using the Random strategy, though you are welcome to switch back and fourth.\n",
    "\n",
    "<img src='figures/HPO_motivation.png' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SageMaker Account & Session Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get things rolling lets make sure we can query our SageMaker execution role and session as well as our account ID and AWS region [ we'll need this info later on ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sm_execution_role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account=!(aws sts get-caller-identity --query Account --output text)\n",
    "region=!(aws configure get region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we'll utilize the Airline dataset (Carrier On-Time Performance 1987-2020, available from the [Bureau of Transportation Statistics](https://transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time#)). \n",
    "\n",
    "Specifically we'll try to classify whether a flight is going to be more than 15 minutes late on arrival. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each flight the features in the data include information about time, the airline, source and destination airports, distance, and departure delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a cleaned version of our dataset on a public S3 bucket, which we specify here and will subsequently use as an input to our HPO Estimators.\n",
    "> Optional Extension: BYOData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bucket = 'rapids-csp'\n",
    "target_bucket_prefix = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_training = 's3://{}/{}'.format(target_bucket, target_bucket_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HPO Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a RAPIDS enabled SageMaker HPO we first need to build an Estimator. \n",
    "\n",
    "An Estimator is a docker container image that captures all the software needed to run an HPO experiment.\n",
    "\n",
    "The container is augmented with special **entrypoint code** that will be triggered at runtime by each worker. \n",
    "\n",
    "The entrypoint code enables us to write custom models and hook them up to data. \n",
    "\n",
    "In order to work with SageMaker HPO, the entrypoint logic should parse hyper-parameters (supplied by SageMaker), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyper-parameter setting.\n",
    "\n",
    "We've already built sample entrypoint code leveraging the cuml.RandomForest classifier model. If you would like to make changes by adding your custom model logic feel free to modify the **train.py** file.\n",
    "\n",
    "<img src='figures/estimator.png' width='85%'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 - Prepare To Build Containerized Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us come back to the custom code in a bit, and assume we have a working implementation. \n",
    "\n",
    "For now lets focus on how we can build our container so that it will fit with the SageMaker HPO API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our container takes the latest RAPIDS [ nightly ] image as a starting layer, adds some bits to inter-operate with SageMaker (i.e., github.com/aws/sagemaker-containers), and copies in custom entypoint code that will run when the Estimator is spawned. We'll discuss the custom logic in the section below, for now lets actually build our container and push it to the Amazon Elastic Container Registry (ECR). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Container Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets decide on the full name of our container `image_base:image_tag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = 'sagemaker-rapids-cloud-ml'\n",
    "image_tag = '0.13_10.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_fullname=f\"{account[0]}.dkr.ecr.{region[0]}.amazonaws.com/{image_base}:{image_tag}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure we have the latest bits by pulling the nightly RAPDIS build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull rapidsai/rapidsai-nightly:0.13-cuda10.1-base-ubuntu18.04-py3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - Write Dockerfile\n",
    "We write out the Dockerfile in this cell, write it to disk, and in the next cell execute the docker build command.\n",
    "> Note that we're copying in custom logic [ train.py, rapids_csp. py ] that we'll be defining shortly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile\n",
    "FROM rapidsai/rapidsai-nightly:0.13-cuda10.1-base-ubuntu18.04-py3.7\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE \\\n",
    "    PYTHONDONTWRITEBYTECODE=TRUE \\\n",
    "    CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential\n",
    "RUN source activate rapids && pip install sagemaker-containers\n",
    "\n",
    "COPY container/rapids_csp.py $CLOUD_PATH/rapids_csp.py\n",
    "COPY container/train.py $CLOUD_PATH/train.py\n",
    "ENV SAGEMAKER_PROGRAM $CLOUD_PATH/train.py\n",
    "\n",
    "WORKDIR $CLOUD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!docker build . --tag $ecr_fullname -f container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 - Push/Publish Container to Elastic Cloud Registry (ECR)\n",
    "Now that we've built and tagged our container its time to push it to Amazon's container registry (ECR). Once in ECR SageMaker will be able to leverage our image to build Estimators and run experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Login to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_login_str = !(aws ecr get-login --region {region[0]} --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{docker_login_str[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ECR repository [ if it doesn't already exist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_query = !(aws ecr describe-repositories --repository-names $image_base)\n",
    "if repository_query[0] == '':\n",
    "    !(aws ecr create-repository --repository-name $image_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Build Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built our container [ +custom logic] and pushed it to ECR, we can finally compile all of efforts into an **Estimator** object -- you can think of the Estimator as the software stack that SageMaker will replicate to each worker node.\n",
    "\n",
    "We'll build the Estimator using our SageMaker execution role, the ECR image we built/tagged, and add an output path to [optionally] save models trained during the HPO experimentation.\n",
    "\n",
    "For additional options and details see the [Estimator documentation](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator) (e.g., to change the size in GB of the EBS volume to use for storing input data during training, default = 30GB )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type_GPU = 'ml.p3.2xlarge' # 'ml.g4dn.4xlarge'\n",
    "train_instance_type_CPU = 'ml.c5.4xlarge'\n",
    "\n",
    "train_instance_type = train_instance_type_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_estimator = sagemaker.estimator.Estimator( sagemaker_session = sm_session, \n",
    "                                              role = sm_execution_role,\n",
    "                                              image_name = ecr_fullname,\n",
    "                                              train_instance_count = 1, \n",
    "                                              train_instance_type = train_instance_type,                                               \n",
    "                                              input_mode = 'File', \n",
    "                                              output_path = f's3://{target_bucket}/{target_bucket_prefix}/output' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Estimator [ optional ]\n",
    "Now that we have a SageMaker Estimator built up, we can feed it data and ask it to train. \n",
    "\n",
    "This is a useful step if you've made changes to your custom logic and are interested in making sure everything works before launching a large HPO search. \n",
    "\n",
    "To trigger this debugging logic  just uncomment and run the cell below.\n",
    "> Note: This verification step will use the default hyper-parameter values declared in our custom train code, as SageMaker HPO will not be orchestrating this single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm_estimator.fit(inputs = s3_input_training, job_name = 'estimator-test-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define HPO\n",
    "With a working SageMaker Estimator in hand, the hardest part is behind us!\n",
    "\n",
    "Now all we have to do is tell SageMaker about the space of hyper-parameters in which to search for the best model.\n",
    "\n",
    "For more documentation check out the SageMaker [HyperParameter Tuner documentation](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 - Defining Search Ranges for HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important choices when running HPO is to choose the bounds of the hyper-parameter search process. \n",
    "\n",
    "Below we've set the ranges of the hyper-parameters to allow for significant variation in all of the different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "from sagemaker.parameter import ContinuousParameter, IntegerParameter, ParameterRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_hyperparameter_ranges = {\n",
    "    'max_depth'    : IntegerParameter    ( 5,  19  ),\n",
    "    'n_estimators' : IntegerParameter    ( 50, 500 ),\n",
    "    'max_features' : ContinuousParameter ( 0.2, 1.0 ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO - Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[{'Name': 'train-accuracy', 'Regex': 'train-accuracy: (.*);'},\n",
    "                    {'Name': 'test-accuracy', 'Regex': 'test-accuracy: (.*);'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'test-accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO - Define Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_experiment = {\n",
    "    'model_type' : 'rf', \n",
    "    'dataset' : 'airline',\n",
    "    'dataset_samples' : 20000000,\n",
    "    'compute_type': 'GPU',\n",
    "    'strategy': 'Random',\n",
    "    'sm_estimator' : sm_estimator,\n",
    "    'metric_definitions' : metric_definitions,\n",
    "    'objective_metric_name' : objective_metric_name,\n",
    "    'hyperparameter_ranges' : random_forest_hyperparameter_ranges,\n",
    "    's3_input_training' : s3_input_training,    \n",
    "    'objective_type': 'Maximize', \n",
    "    'max_jobs': 10,\n",
    "    'max_parallel_jobs': 2,\n",
    "    'CV_folds' : 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = sagemaker.tuner.HyperparameterTuner( estimator = HPO_experiment['sm_estimator'],\n",
    "                                           metric_definitions = HPO_experiment['metric_definitions'], \n",
    "                                           objective_metric_name = HPO_experiment['objective_metric_name'],\n",
    "                                           objective_type = HPO_experiment['objective_type'],\n",
    "                                           hyperparameter_ranges = HPO_experiment['hyperparameter_ranges'],\n",
    "                                           strategy = HPO_experiment['strategy'],  \n",
    "                                           max_jobs = HPO_experiment['max_jobs'],\n",
    "                                           max_parallel_jobs = HPO_experiment['max_parallel_jobs'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figures/max_jobs.png' width='800px'>\n",
    "<img src='figures/max_parallel.png' width='500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build HPO Job Name \n",
    "Using these HPO parameters we'll build up a unique name for this HPO job. \n",
    "> Note that we'll be using the name to determine some of the custom logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tag = 'v100'\n",
    "HPO_experiment['experiment_name'] = f\"{HPO_experiment['model_type']}-{HPO_experiment['compute_type']}-CV-{HPO_experiment['CV_folds']}-{HPO_experiment['dataset_samples']}-{custom_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = HPO_experiment['experiment_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "hpo.fit( inputs = HPO_experiment['s3_input_training'], \n",
    "         job_name = HPO_experiment['experiment_name'], wait = True, logs = 'All')    \n",
    "hpo.wait() # block until the .fit call above is completed\n",
    "\n",
    "HPO_job_total_time = time.perf_counter() - start_time\n",
    "print(HPO_job_total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: describe choices (e.g., instance type), add summary, finalize/confirm defaults with team, add notes on instance limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "AWS SageMaker + NVIDIA RAPIDS HPO FTW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Metric Parsing [ Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
