{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/header.png' width='75%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Optimization (HPO) imporves model quality by searching the space of possible 'architecture parameters,' parameters not usually trained during the learning process.  This search can significantly boost model quality relative to default parameters and non-expert tuning; however, the search over architectures can take a very long time on a non-accelerated platform. In this notebook, we containerize a RAPIDS workflow and run Bring-Yor-Own-Container SageMaker HPO to show how we can overcome the computational complexity of model search. \n",
    "\n",
    "We accelerate HPO in two key ways: \n",
    "* by *scaling within a node* (e.g., multi-GPU where each GPU brings a magnitude higher core count relative to CPUs), and \n",
    "* by *scaling across nodes* and running parallel trials on cloud instances.\n",
    "\n",
    "By combining these two powers HPO experiments that feel unapproachable and may take multiple days on CPU instances can complete in just hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/results.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, when comparing between GPU and CPU instances on 100 HPO experiments using 10 parallel workers, XGBoost with 10 cross-validation folds, and 10 years of the Airline Dataset (~63M flights) we found a <span style=\"color:#8735fb; font-size:14pt\"> **12X** </span> speedup in wall clock time and a <span style=\"color:#8735fb; font-size:14pt\"> **4.5x** </span> reduction in cost. With all these powerful tools at our disposal, every data scientist should feel empowered to uplevel their model before serving it to the world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/hpo.png' widht='85%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Key Choices** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and choose the configuration options for our HPO run.\n",
    "\n",
    "Note that we've set the configuration defaults to values that are on the low end, however you are welcome to scale them up. \n",
    "\n",
    "Here are two reference configurations showing a small and a large scale HPO.\n",
    "\n",
    "> sample small HPO config : 1_year, XGBoost, 3 CV folds, singleGPU, max_jobs = 10, max_parallel_jobs = 2\n",
    "\n",
    "> sample big HPO config: 10_year, XGBoost, 10 CV folds, multiGPU, max_jobs = 100, max_parallel_jobs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Dataset Size and S3 Bucket ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We target a large real-world structured dataset or flight logs for US airlines and train a model to predict flight delays ( published monthly since 1987 by the Bureau of Transportation [dataset link](https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&DB_URL=)). \n",
    "\n",
    "We host 3 increasingly larger versions of this dataset as directoreis in a public bucket, and offer `1_year` (2019, 7.2M flights), `3_year` (2016-2019, 18M flights) or `10_year` (2009-2019, 125M flights) configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = '1_year'\n",
    "\n",
    "assert( dataset_directory in [ '1_year', '3_year', '10_year'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_URI = f\"s3://{'sagemaker-rapids-hpo-us-east-1'}/{dataset_directory}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Algorithm ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a ML/algorithm perspective, we offer `XGBoost` and `RandomForest` decision tree models which do quite well on this structured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_choice = 'XGBoost'\n",
    "\n",
    "assert ( algorithm_choice in [ 'XGBoost', 'RandomForest' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also optionally increase robustness via reshuffles of the train-test split (i.e., cross-validation folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 3\n",
    "\n",
    "assert ( cv_folds >= 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Code ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable the option of running different code variations that unlock increasing amounts of parallelism in the compute workflow.\n",
    "\n",
    "* `singleCPU`* = pandas + sklearn\n",
    "* `multiCPU`   = dask + pandas + sklearn\n",
    "* `singleGPU` = cudf + cuml \n",
    "* `multiGPU`  = dask + cudf + cuml \n",
    "\n",
    "Note that the single-CPU option still uses multiple cores in the model training portion of the workflow; however, to unlock parallelism in each stage of the workflow we use dask. We've integrated all to of these options in the rapids_cloud_ml.py file, we also ofer individual notebooks that look at the variations in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_choice = 'singleGPU' \n",
    "\n",
    "assert ( code_choice in [ 'singleCPU', 'singleGPU', 'multiCPU', 'multiGPU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Search Strategy and Ranges ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important choices when running HPO is to choose the bounds of the hyper-parameter search process. Below we've set the ranges of the hyper-parameters to allow for significant variation in all of the different dimensions and we've selected the Random search strategy. You are of course welcome to modify these settings and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_strategy = 'Random'\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth'    : sagemaker.parameter.IntegerParameter        ( 5, 15 ),\n",
    "    'n_estimators' : sagemaker.parameter.IntegerParameter        ( 100, 500 ),\n",
    "    'max_features' : sagemaker.parameter.ContinuousParameter     ( 0.1, 1.0 ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ( search_strategy in [ 'Random', 'Bayesian' ])\n",
    "if 'XGBoost' in algorithm_choice:    \n",
    "    hyperparameter_ranges['num_boost_round'] = hyperparameter_ranges.pop('n_estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Experiment Scale ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to decide how may total experiments to run, and how many should run in parallel.\n",
    "> Note that you may need to request an instance limit increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_jobs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set the max duration for an individual job to 24 hours so we don't have run-away compute jobs taking too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration_of_experiment_seconds = 60*60*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Compute Platform ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset size and compute choice we will try to recommend an instance choice*, you are of course welcome to select alternate configurations. \n",
    "> e.g., we recommend ml.m5.24xlarge CPU instances for the 10_year variations since we'll need upwards of 200GB during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended instance type : ml.p3.2xlarge \n",
      "instance details          : 1x V100, 16GB GPU memory, 61GB CPU memory\n"
     ]
    }
   ],
   "source": [
    "instance_type = recommend_instance_type ( code_choice, dataset_directory  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Validate** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 uri           =\ts3://sagemaker-rapids-hpo-us-east-1/1_year\n",
      "compute          =\tsingleGPU\n",
      "algorithm        =\tXGBoost, 3 cv-fold\n",
      "instance         =\tml.p3.2xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t10\n",
      "max_parallel     =\t2\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices( s3_data_URI, code_choice, algorithm_choice, cv_folds,\n",
    "                   instance_type, use_spot_instances_flag, search_strategy, \n",
    "                   max_jobs, max_parallel_jobs, max_duration_of_experiment_seconds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **1. ML Workflow** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/ml_workflow.png' width='800'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 1.1 - Dataset </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we'll utilize the Airline dataset (Carrier On-Time Performance 1987-2020, available from the [Bureau of Transportation Statistics](https://transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time#)). \n",
    "\n",
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* locations and distance  ( `Origin`, `Dest`, `Distance` )\n",
    "* airline / carrier ( `Reporting_Airline` )\n",
    "* scheduled departure and arrival times ( `CRSDepTime` and `CRSArrTime` )\n",
    "* actual departure and arrival times ( `DpTime` and `ArrTime` )\n",
    "* difference between scheduled & actual times ( `ArrDelay` and `DepDelay` )\n",
    "* binary encoded version of late, aka our target variable ( `ArrDelay15` )\n",
    "\n",
    "Using these features we'll be trying to build a classifier model to predict whether a flight is going to be more than 15 minutes late on arrival as it prepares to depart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 1.2 - Python DS Workflow [ ETL, Train, Eval ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a RAPIDS enabled SageMaker HPO we first need to build an Estimator. An Estimator is a container image that captures all the software needed to run an HPO experiment. The container is augmented with entrypoint code that will be trggered at runtime by each worker. The entrypoint code enables us to write custom models and hook them up to data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with SageMaker HPO, the entrypoint logic should parse hyper-parameters (supplied by AWS SageMaker), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyper-parameter setting. We've already built multiple variations of this code leveraging. If you would like to make changes by adding your custom model logic feel free to modify the **train.py** and **rapids_cloud_ml.py** files in the code directory. Uncomment the cells below to load the read/review the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../code/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../code/rapids_cloud_ml.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **2. Build Estimator** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/estimator.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already mentioned, the SageMaker Estimator represents the containerized software stack that AWS SageMaker will replicate to each worker node.\n",
    "\n",
    "The first step to building our Estimator, is to augment a RAPIDS container with our ML Workflow code from above, and push this image to Amazon Elastic Cloud Registry so it is available to SageMaker.\n",
    "\n",
    "For additional options and details see the [Estimator documentation](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.1 - Containerize and Push to ECR </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets turn to building our container so that it can integrate with the AWS SageMaker HPO API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our container can either be built on top of the latest RAPIDS [ nightly ] image as a starting layer or the RAPIDS stable image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapids_stable = 'rapidsai/rapidsai:0.14-cuda10.1-runtime-ubuntu18.04-py3.7'\n",
    "rapids_nightly = 'rapidsai/rapidsai-nightly:0.15-cuda10.1-runtime-ubuntu18.04-py3.7'\n",
    "\n",
    "rapids_base_container = rapids_stable\n",
    "assert ( rapids_base_container in [ rapids_stable, rapids_nightly ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get things rolling lets make sure we can query our AWS SageMaker execution role and session as well as our account ID and AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "account=!(aws sts get-caller-identity --query Account --output text)\n",
    "region=!(aws configure get region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['561241433344'], ['us-east-1'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account, region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide on the full name of our container `image_base:image_tag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = 'cloud-ml-sagemaker'\n",
    "image_tag  = rapids_base_container.split(':')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_fullname = f\"{account[0]}.dkr.ecr.{region[0]}.amazonaws.com/{image_base}:{image_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:0.15-cuda10.1-runtime-ubuntu18.04-py3.7'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.1 - Write Dockerfile </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write out the Dockerfile in this cell, write it to disk, and in the next cell execute the docker build command.\n",
    "> Note that we're copying in custom logic [ train.py, rapids_csp. py ] that we'll be defining shortly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir='~/SageMaker/cloud-ml-examples/aws/code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/cloud-ml-examples/aws/code\n"
     ]
    }
   ],
   "source": [
    "%cd {workdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our selected RAPDIS image layer as the first FROM statement in the the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dockerfile', 'w') as dockerfile_handle: \n",
    "    dockerfile_handle.writelines( 'FROM ' + rapids_base_container + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets write the remaining pieces of the Dockerfile, namely adding the sagemaker-training-toolkit and copying our python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Dockerfile\n",
    "\n",
    "# install https://github.com/aws/sagemaker-training-toolkit\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential \\ \n",
    "    && source activate rapids && pip3 install sagemaker-training\n",
    "\n",
    "# path where sagemaker looks for our code\n",
    "ENV CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "# copy our latest [local] code into the container \n",
    "COPY rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
    "COPY train.py $CLOUD_PATH/train.py\n",
    "\n",
    "# sagemaker entrypoint will be train.py\n",
    "ENV SAGEMAKER_PROGRAM train.py \n",
    "\n",
    "WORKDIR $CLOUD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's ensure that our Dockerfile correctly captured our base image selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM rapidsai/rapidsai-nightly:0.15-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      "\n",
      "# install https://github.com/aws/sagemaker-training-toolkit\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends build-essential \\ \n",
      "    && source activate rapids && pip3 install sagemaker-training\n",
      "\n",
      "# path where sagemaker looks for our code\n",
      "ENV CLOUD_PATH=\"/opt/ml/code\"\n",
      "\n",
      "# copy our latest [local] code into the container \n",
      "COPY rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
      "COPY train.py $CLOUD_PATH/train.py\n",
      "\n",
      "# sagemaker entrypoint will be train.py\n",
      "ENV SAGEMAKER_PROGRAM train.py \n",
      "\n",
      "WORKDIR $CLOUD_PATH\n"
     ]
    }
   ],
   "source": [
    "validate_dockerfile( rapids_base_container )\n",
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.2 Build and Tag </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build usually take less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  30.21kB\n",
      "Step 1/7 : FROM rapidsai/rapidsai-nightly:0.15-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      " ---> 0aa868e59c3b\n",
      "Step 2/7 : RUN apt-get update && apt-get install -y --no-install-recommends build-essential     && source activate rapids && pip3 install sagemaker-training\n",
      " ---> Using cache\n",
      " ---> d4dcac8e3726\n",
      "Step 3/7 : ENV CLOUD_PATH=\"/opt/ml/code\"\n",
      " ---> Using cache\n",
      " ---> cdb78892718a\n",
      "Step 4/7 : COPY rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
      " ---> Using cache\n",
      " ---> 4fe5d342ef25\n",
      "Step 5/7 : COPY train.py $CLOUD_PATH/train.py\n",
      " ---> Using cache\n",
      " ---> 07ca1ddfee71\n",
      "Step 6/7 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Using cache\n",
      " ---> 6c0c5ad71ece\n",
      "Step 7/7 : WORKDIR $CLOUD_PATH\n",
      " ---> Using cache\n",
      " ---> c7334998c616\n",
      "Successfully built c7334998c616\n",
      "Successfully tagged 561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:0.15-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      "CPU times: user 9.2 ms, sys: 4.68 ms, total: 13.9 ms\n",
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!docker build . -t $ecr_fullname -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.3 - Publish to Elastic Cloud Registry (ECR) </span>\n",
    "\n",
    "Now that we've built and tagged our container its time to push it to Amazon's container registry (ECR). Once in ECR, AWS SageMaker will be able to leverage our image to build Estimators and run experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker Login to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_login_str = !(aws ecr get-login --region {region[0]} --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!{docker_login_str[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository [ if it doesn't already exist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_query = !(aws ecr describe-repositories --repository-names $image_base)\n",
    "if repository_query[0] == '':\n",
    "    !(aws ecr create-repository --repository-name $image_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now actually push the container to ECR\n",
    "> Note the first push to ECR may take some time (hopefully less than 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:0.15-cuda10.1-runtime-ubuntu18.04-py3.7'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker]\n",
      "\n",
      "\u001b[1Bc37f3422: Preparing \n",
      "\u001b[1B3214f96f: Preparing \n",
      "\u001b[1B6b4e1e7d: Preparing \n",
      "\u001b[1B6ce3e91a: Preparing \n",
      "\u001b[1B70bf2e4d: Preparing \n",
      "\u001b[1B4e7152cf: Preparing \n",
      "\u001b[1B74f76be4: Preparing \n",
      "\u001b[1Bd332a58a: Preparing \n",
      "\u001b[1Bf11cbf29: Preparing \n",
      "\u001b[1Ba4b22186: Preparing \n",
      "\u001b[1Bafb09dc3: Preparing \n",
      "\u001b[1Bb5a53aac: Preparing \n",
      "\u001b[1Bc8e5063e: Preparing \n",
      "\u001b[2Bc8e5063e: Layer already exists \u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K0.15-cuda10.1-runtime-ubuntu18.04-py3.7: digest: sha256:484b892fec17f5dacd32e5de06ee3a09b66c4a7094074399fab04a7f0dae552d size: 3265\n"
     ]
    }
   ],
   "source": [
    "!docker push $ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.2 - Create Estimator </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built our container [ +custom logic] and pushed it to ECR, we can finally compile all of efforts into an Estimator instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_params = {\n",
    "    'image_name' : ecr_fullname,\n",
    "    \n",
    "    'train_instance_type' : instance_type, \n",
    "    'train_instance_count' : 1, \n",
    "    'train_use_spot_instances': use_spot_instances_flag,\n",
    "    \n",
    "    'train_max_run' : 60*60*24, # 24 hours\n",
    "    'train_max_wait' : 60*60*24 + 1,  \n",
    "    \n",
    "    'input_mode' : 'File',\n",
    "    'sagemaker_session' : session,     \n",
    "    'role' : execution_role,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator( **estimator_params  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.3 - Test Estimator </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test by asking SageMaker to run the BYOContainer logic inside our Estimator. This is a useful step if you've made changes to your custom logic and are interested in making sure everything works before launching a large HPO search. \n",
    "\n",
    "> Note: This verification step will use the default hyper-parameter values declared in our custom train code, as SageMaker HPO will not be orchestrating a search for this single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 uri           =\ts3://sagemaker-rapids-hpo-us-east-1/1_year\n",
      "compute          =\tsingleGPU\n",
      "algorithm        =\tXGBoost, 3 cv-fold\n",
      "instance         =\tml.p3.2xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t10\n",
      "max_parallel     =\t2\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices( s3_data_URI, code_choice, algorithm_choice, cv_folds,\n",
    "                   instance_type, use_spot_instances_flag, search_strategy, \n",
    "                   max_jobs, max_parallel_jobs, max_duration_of_experiment_seconds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ( input('confirm test run? [ y / n ] : ').lower() == 'y' )\n",
    "\n",
    "job_name = new_job_name_from_config( dataset_directory, code_choice, \n",
    "                                     algorithm_choice, cv_folds,\n",
    "                                     instance_type  )\n",
    "\n",
    "estimator.fit(inputs = s3_data_URI, job_name=job_name.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **3. Run HPO** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/run_hpo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a working SageMaker Estimator in hand, the hardest part is behind us. Now all we have to do is tell SageMaker about the space of hyper-parameters in which to search for the best model.\n",
    "\n",
    "For more documentation check out the AWS SageMaker [HyperParameter Tuner documentation](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.1 - Define Metric </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions below specify a regular expressions (i.e., string parsing rules) to find the metrics which we are using to evalaute performance in the output log of each worker/Estimator. In this case we are case we are onyl interested in the performance of our model on the test data (i.e., `test-accuracy`), so we have a single metric to track.\n",
    "\n",
    "For additional details on metrics refer to the [AWS SageMaker documentation on Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{'Name': 'final-score', 'Regex': 'final-score: (.*);'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'final-score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.2 - Define Tuner </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are setting up the parameters that will define the HPO job. By default (to avoid accidently spawning large compute jobs), we have limited the number of HPO experiments to run to 2.\n",
    "\n",
    "To run a more realistic large-scale HPO, change `max_jobs` to 100 and `max_parallel_jobs` to 10 (or as high as your instance limit permits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = sagemaker.tuner.HyperparameterTuner( estimator = estimator,\n",
    "                                           metric_definitions = metric_definitions, \n",
    "                                           objective_metric_name = objective_metric_name,\n",
    "                                           objective_type = 'Maximize',\n",
    "                                           hyperparameter_ranges = hyperparameter_ranges,\n",
    "                                           strategy = search_strategy,  \n",
    "                                           max_jobs = max_jobs,\n",
    "                                           max_parallel_jobs = max_parallel_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.3 - Run HPO </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_choices( s3_data_URI, code_choice, algorithm_choice, cv_folds,\n",
    "                   instance_type, use_spot_instances_flag, search_strategy, \n",
    "                   max_jobs, max_parallel_jobs, max_duration_of_experiment_seconds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure we take a moment to confirm before launching all of our HPO experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ( input('confirm HPO launch? [ y / n ] : ').lower() == 'y' )\n",
    "\n",
    "tuning_job_name = new_job_name_from_config( dataset_directory, code_choice, \n",
    "                                            algorithm_choice, cv_folds, \n",
    "                                            instance_type )\n",
    "hpo.fit( inputs = s3_data_URI, \n",
    "         job_name = tuning_job_name, \n",
    "         wait = True, logs = 'All') \n",
    "\n",
    "hpo.wait() # block until the .fit call above is completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.4 - Results and Summary </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS SageMaker + NVIDIA RAPIDS HPO FTW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> Rapids References </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [cloud-ml-examples](http://github.com/rapidsai/cloud-ml-examples)\n",
    "\n",
    "> [RAPIDS HPO](https://rapids.ai/hpo)\n",
    "\n",
    "> [cuML Documentation](https://docs.rapids.ai/api/cuml/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> SageMaker References </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit)\n",
    "\n",
    "> [Estimator Parameters](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n",
    "\n",
    "> Spot Instances [docs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html), and [blog]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
